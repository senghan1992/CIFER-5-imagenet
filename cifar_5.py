# -*- coding: utf-8 -*-
"""CIFAR-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qRpyJE40bPqPTbo24ECD31rpac97Y4fZ
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
import requests
import cv2
import urllib
import os
import os.path
import PIL.ImageOps

from torch import nn
from torchvision import transforms, datasets, models
from bs4 import BeautifulSoup
from PIL import Image

"""#### Imagenet api urls"""


def url_to_image(url):
    resp = urllib.request.urlopen(url)
    image = np.asarray(bytearray(resp.read()), dtype="uint8")
    image = cv2.imdecode(image, cv2.IMREAD_COLOR)
    return image

classes = []
for i in os.listdir("./data/train"):
    classes.append(i)

#classes = classes.sort(reverse=False)
#print(type(classes))
#print(classes.sort)


classes = ['bicycles', 'cars', 'cups', 'laptops', 'mobiles']

imagenet_urls = [
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n04037443",
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02835271",
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03733805",
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03642806",
    "http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02992529"
]

# img_rows, img_cols = 32,32
# input_shpae = (3, img_rows, img_cols)

for idx, image_url in enumerate(imagenet_urls):
    print("<<<< %s image get start >>>>" % classes[idx])
    # convert url to image
    page = requests.get(image_url)
    soup = BeautifulSoup(page.content, 'html.parser')
    str_soup = str(soup)
    split_urls = str_soup.split('\r\n')
    # print(len(split_urls))
    ########################################
    # training data
    # int(len(split_urls) * 0.7)  # training image 갯수
    n_of_training_images = 200
    # print(n_of_training_images)
    for progress in range(n_of_training_images):
        if not split_urls[progress] == None:
            try:
                save_path = "./data/train/%s/%s_%d.jpg" % (
                    classes[idx], classes[idx], progress)
                if not os.path.isfile(save_path):
                    I = url_to_image(split_urls[progress])
                    if (len(I.shape)) == 3:
                        # print(save_path)
                        cv2.imwrite(save_path, I)
                        print("saved file : %s" % save_path)
                    if progress % 20 == 0:
                        print("train data %s" % progress)
                        print(I.shape)
                        print("filename:%s" % save_path)
            except:
                None
        #############################################
        # validation data
        # n_of_validation_images = int(len(split_urls) * 0.3) # validation image 갯수
    else:
        for progress in range(70):
            if not split_urls[progress] == None:
                try:
                    save_path = "./data/valid/%s/%s_%d.jpg" % (
                        classes[idx], classes[idx], n_of_training_images + progress)
                    if not os.path.isfile(save_path):
                        I = url_to_image(
                            split_urls[n_of_training_images + progress])
                        if (len(I.shape)) == 3:
                            cv2.imwrite(save_path, I)
                            print("saved file : %s" % save_path)
                        if progress % 20 == 0:
                            print("valid data %s" % progress)
                            print(I.shape)
                            print("filename:%s" % save_path)
                except:
                    None

"""### transform setting"""
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

training_dataset = datasets.ImageFolder(
    "./data/train", transform=transform_train)
validation_dataset = datasets.ImageFolder(
    "./data/valid", transform=transform)

training_loader = torch.utils.data.DataLoader(
    training_dataset, batch_size=20, shuffle=True)
validation_loader = torch.utils.data.DataLoader(
    validation_dataset, batch_size=20, shuffle=False)

'''
print(len(training_dataset))
print(len(validation_dataset))
print(len(training_loader))
print(len(validation_loader))
'''


def im_convert(tensor):
    image = tensor.cpu().clone().detach().numpy()
    # image = image.squeeze(0)
    image = image.transpose(1, 2, 0)
    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))
    image = image.clip(0, 1)
    return image


'''
class LeNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1)
        self.conv3 = nn.Conv2d(32, 64, 3, 1, padding=1)
        self.fc1 = nn.Linear(4*4*64, 500)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(500, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(x, 2, 2)

        x = x.view(-1, 4*4*64)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x
'''

model = models.vgg16(pretrained=True)
for param in model.features.parameters():
    param.requires_grad = False

n_inputs = model.classifier[6].in_features
last_layer = nn.Linear(n_inputs, len(classes))
model.classifier[6] = last_layer
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

epochs = 10
running_loss_history = []
running_corrects_history = []
val_running_loss_history = []
val_running_corrects_history = []

for e in range(epochs):
    running_loss = 0.0
    running_corrects = 0.0
    val_running_loss = 0.0
    val_running_corrects = 0.0

    for inputs, labels in training_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)
        running_loss += loss.item()
        running_corrects += torch.sum(preds == labels.data)

    else:
        with torch.no_grad():
            for val_inputs, val_labels in validation_loader:
                val_inputs = val_inputs.to(device)
                val_labels = val_labels.to(device)
                val_outputs = model(val_inputs)
                val_loss = criterion(val_outputs, val_labels)

                _, val_preds = torch.max(val_outputs, 1)
                val_running_loss += val_loss.item()
                val_running_corrects += torch.sum(val_preds == val_labels.data)

            epoch_loss = running_loss / len(training_loader.dataset)
            epoch_acc = running_corrects.float() / len(training_loader.dataset)
            running_loss_history.append(epoch_loss)
            running_corrects_history.append(epoch_acc)

            val_epoch_loss = val_running_loss / len(validation_loader.dataset)
            val_epoch_acc = val_running_corrects.float() / len(validation_loader.dataset)
            val_running_loss_history.append(val_epoch_loss)
            val_running_corrects_history.append(val_epoch_acc)

            print("epoch:", (e+1))
            print("training loss: {:.4f}, acc {:.4f} ".format(
                epoch_loss, epoch_acc.item()))
            print("validation loss: {:.4f}, validation acc {:.4f} ".format(
                val_epoch_loss, val_epoch_acc))

# 모르는 사진으로 검증
url = "https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQa5HJ4_NHpVmrbT2fT88KOXCX0c93iXzT9ZbOahDhPUkgrj_mf"
response = requests.get(url, stream=True)
img = Image.open(response.raw)
img = transform(img)

image = img.to(device).unsqueeze(0)
output = model(image)
_, pred = torch.max(output, 1)
#classes = ['bicycles','cars','cups', 'laptops', 'mobiles']
print(classes)
print(pred.item())
print(classes[pred.item()])

#model.save_state_dict('mytraining.pt')

# write log
text = open('./data/vgg16_results.txt', 'w')
for idx, running_loss_history_item in enumerate(running_loss_history):
    data1 = "running_correct_history(%d): %f\n" % (
        idx, running_corrects_history[idx])
    data2 = "val_running_correct_history(%d): %f\n" % (
        idx, val_running_corrects_history[idx])
    text.write(data1)
    text.write(data2)

text.write("answer: Cars / guess: %s\n" % classes[pred.item()])
text.close()

#torch.save(model.state_dict(), './data/')